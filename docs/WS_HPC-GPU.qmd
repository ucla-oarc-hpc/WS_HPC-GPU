---
title: "Optimizing research with GPUs on Hoffman2"
author: "Charles Peterson"
format: 
  revealjs: 
    transition: slide
    theme: [custom.scss]
    scrollable: true
    self-contained: true
from: markdown+emoji
---

# :wave: Welcome Everyone!

## :dart: Workshop Overview

::: {style="font-size: 0.70em" }

:rocket: Discover the power of GPU computing to accelerate your research on UCLA's Hoffman2 cluster! This beginner-friendly workshop will guide you through the basics of GPU utilization, enhancing your projects with cutting-edge computational efficiency. :star:


:::
:::: {.columns .fragment}
::: {.column width="60%"}
::: {style="font-size: 0.70em" }

:point_right: What you'll learn:

- :brain: **Understanding GPU architecture** and its benefits
- :computer: **Hands-on access** to Hoffman2's advanced GPU resources
- :snake: **Utilizing Python and R** for GPU computing
- **RPyLab** - A container with RStudio and Jupyter with GPU support (experimental)

For suggestions: [cpeterson\@oarc.ucla.edu](mailto:cpeterson@oarc.ucla.edu){.email}

:::
:::
::: {.column width="40%"}
::: {style="text-align: center"}

<img src="fullpic.png"/ width="50%">

:::
:::
::::

## :open_book: Access the Workshop Files

::: {style="font-size: 0.75em" }

This presentation and accompanying materials are available on :link: [UCLA OARC GitHub Repository](https://github.com/ucla-oarc-hpc/WS_HPC-GPU)

You can view the slides in:

- :page_facing_up: PDF format - WS_HPC-GPU.pdf
- :globe_with_meridians: HTML format: [Workshop Slides](https://ucla-oarc-hpc.github.io/WS_HPC-GPU)
- :movie_camera: Recordings can be found on our [BOX account](https://ucla.box.com/s/6fsinqhvsv3ywf9wmg30565iw7l8wvt1)

> **Note:** :hammer_and_wrench: This presentation was built using [Quarto](https://quarto.org/) and RStudio.

:::: {.columns}
::: {.column width="35%"}

Clone repository to access the workshop files:

:::
::: {.column width="65%"}
```{.bash}
git clone https://github.com/ucla-oarc-hpc/WS_HPC-GPU.git
```

:::
::::
:::

# :computer: GPU Basics

## :thinking: What are Graphic Processing Units?

::: {style="font-size: 0.65em" }

- Initially developed for processing graphics and visual operations
   - CPUs were too slow for these tasks
   - Architecture of GPUs allows to handle massively parallel tasks efficiently
   - Found in everything from PCs, mobile phones, gaming consoles, and more

:rocket: In the mid-2000s, GPUs began to be used for non-graphical computations. NVIDIA introduced CUDA, a programming language that allows for compiling non-graphic programs on GPUs, spearheading the era of General-Purpose GPU (GPGPU).

:::: {.columns}
::: {.column width="25%"}

GeForce 256

- First 'GPU' in 1999
- 32 MB of memory
- 960 MFLOPS (FP32)
:::
::: {.column width="25%"}

![](gforce.jpg)
:::
::: {.column width="25%"}

A100

- 80 GB of memory
- 19.5 TFLOPS (FP32)

:::
::: {.column width="25%"}

![](a100.jpeg)
:::
::::
:::

## :globe_with_meridians: Applications of GPUs

::: { style="font-size: 0.75em" }

GPUs are ubiquitous and found in devices ranging from PCs to mobile phones, and gaming consoles like Xbox and PlayStation.

Though initially designed for graphics, GPUs are now used in a wide range of applications.

:::: {.columns}
::: {.column width="60%" }

- :brain: **Machine Learning:** Training and inference especially in Deep Learning neural networks
- :book: **Large Language Models:** Training for NLP models
- :mag: **Data Science:** Accelerating data processing and analysis
- :computer: **High-Performance Computing:** Simulations and scientific computing

:::
::: {.column width="40%"}

![](gpucomputing.png){height=70%}
:::
::::
:::

## :bullettrain_side: GPU Performance


![](gromacspic.png){.absolute top=200 left=0 width="325" height="300"}

![](gromacschart.png){.absolute bottom=0 right=50}

::: {.footer}
picture source GROMACS
:::

## :zap: The Power of GPUs

The significant speedup offered by GPUs comes from their ability to parallelize operations over thousands of cores, unlike traditional CPUs.

:::: {.columns}
::: {.column width="40%"}

![](dataset.png)

:::
::: {.column width="60%"}
![](cpugpu.png)
:::
::::

::: {.footer}
picture source NVIDIA
:::

## :wrench: GPU Workflow

![](codegpuflow.png)

::: {.footer}
picture source NVIDIA
:::

## :man_juggling: GPU considerations

:::: {.columns}
::: {.column width="70%"}
::: {style="font-size: 0.80em" }

- :construction: **Code Optimization:** Some codes are not suitable for GPU.
- :construction_worker: **GPU architecture:** Some codes can run more efficiently on some GPUs over others, or sometimes not at all.
- :arrows_counterclockwise:**Overhead:** Data transfer between CPU and GPU can be costly.
- :brain: **Memory Management:** GPU memory is limited and can be a bottleneck.

:::
:::
::: {.column width="30%"}

![](gpupic.png)
:::
::::


## :chart_with_upwards_trend: GPUs on Hoffman2

::: {style="font-size: 0.65em" }

There are multiple GPU types available in the cluster. Each GPU has a different compute capability, memory size, and clock speed.


| GPU type | # CUDA cores |VMem | SGE option |
|---------|:-----|------:|:------:|
| NVIDIA A100      | 6912   |    80 GB |   -l gpu,A100,cuda=1   |
| Tesla V100     | 5120  |   32 GB |  -l gpu,V100,cuda=1  |
| RTX 2080 Ti       | 4352    |     10 GB |   -l gpu,RTX2080Ti,cuda=1  |
| Tesla P4  |  2560 | 8 GB | -l gpu,P4,cuda=1 |

::: {.fragment}
:::: {.columns }
::: {.column width="30%"}

Interactive job 

:::
::: {.column width="70%"}

```{.bash}
qrsh -l h_data=40G,h_rt=1:00:00,gpu,A100,cuda=1
```

:::
::::
:::: {.columns}
::: {.column width="30%"}

Batch submission 

:::
::: {.column width="70%"}

```{.bash}
#SBATCH -l gpu,A100,cuda=1
```

:::
::::
::: {.callout-note}
If you would like to host GPU nodes on Hoffman2 or get `highp` access, please contact us!
:::
:::
:::

## :gear: GPU optimization

::: {.callout-warning}
When you using the `-l gpu` option, this only reserves the GPU for your job. 

You will still need to use GPU optimized software and libraries to take advantage of the GPU's parallel processing power.

:::

The following sections will cover how to compile and run GPU optimized code on Hoffman2.


# :wrench: Compiling GPU Software

## :jigsaw: CUDA

::: {style="font-size: 0.80em" }

:::: {.columns}
::: {.column width="60%"}

CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model from NVIDIA. It enables developers to write software that harnesses the power of GPUs for more than just graphics — expanding into high-performance computing and deep learning.


:::
::: {.column width="40%"}

![](cuda.jpeg){height=80%}
:::
::::

On Hoffman2, you can compile CUDA code by loading the `cuda` module. This prepares your environment with tools from the [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit), which includes essential libraries and compilers for GPU code execution.


:::: {.columns}
::: {.column width="60%"}

See all available CUDA version

:::
::: {.column width="40%"}

```{.bash}
modules_lookup -m cuda
```
:::
::::
:::: {.columns}
::: {.column width="60%"}
Loading the CUDA 11.8 Toolkit
:::
::: {.column width="40%"}

```{.bash}
module load cuda/11.8
```
:::
::::
:::

## :books: CUDA libraries

![](gpulibraries.png){fig-align="center"}

::: {.footer}
picture source NVIDIA
:::

## :hourglass_flowing_sand: CUDA code example 

![](cudacode.png){fig-align="center"}


## :test_tube: CUDA code example

::: {style="font-size: 0.80em" }

Here’s a simple CUDA code example that performs matrix multiplication (1024x1024):

- Files are in the `MatrixMult` folder
  - `Matrix-cpu.cpp` contains CPU (serial) code
  - `Matrix-gpu.cu` contains the CUDA code
  - `MatrixMult.job` job submission file

:::: {.columns}
::: {.column width="40%"}

Loading required modules

:::
::: {.column width="60%"}

```{.bash}
module load gcc/10.2.0
module load cuda/12.3
```

:::
::::
:::: {.columns}
::: {.column width="40%"}

Compiling code

:::
::: {.column width="60%"}
```{.bash}
g++ -o Matrix-cpu Matrix-cpu.cpp
nvcc -o Matrix-gpu Matrix-gpu.cu
```

:::
::::
:::: {.columns}
::: {.column width="40%"}

Submitting the job

:::
::: {.column width="60%"}

```{.bash}
qsub MatrixMult.job
```

:::
::::
:::

## :computer: GPU software

::: {style="font-size: 0.80em" }

Be on the lookout for GPU optimized software for your research!

Other GPU platforms include:

:::: {.columns}
::: {.column width="50%"}

- [NVIDIA's HPC SDK](https://developer.nvidia.com/hpc-sdk) (Software Developemnt Kit)
   - C/C++/Fortran compilers, Math libraries, and Open MPI

:::
::: {.column width="50%"}

```{.bash}
modules_lookup -m hpcsdk
```

:::
::::
:::: {.columns}
::: {.column width="50%"}

- [AMD ROCm](https://www.amd.com/en/products/software/rocm.html) (Radeon Open Compute)
   - For AMD GPUs

:::
::: {.column width="50%"}

```{.bash}
modules_lookup -m amd
```

:::
::::
:::

   
# Using Python/R for GPU Computing

## GPUs for Python and R

::: { style="font-size: 0.75em"}

There are several Python and R packages that use GPUs for varsious data-intensives tasks, like Machine Learning, Deep Learning, and large-scale data processing.

:::: {.columns}
::: {.column width="50%"}

Python:

- [TensorFlow](https://www.tensorflow.org/): One of the most widely used libraries for machine learning and deep learning that supports GPUs for acceleration.
- [PyTorch](https://pytorch.org/): A popular library for deep learning that features strong GPU acceleration and is favored for its flexibility and speed.
- [cuPy](https://cupy.dev/): A library that provides GPU-accelerated equivalents to NumPy functions, facilitating easy transitions from CPU to GPU.
- [RAPIDS](https://rapids.ai/): A suite of open-source software libraries built on CUDA-X AI, providing the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.
- [Numba](https://numba.pydata.org/): An open-source JIT compiler that translates a subset of Python and NumPy code into fast machine code, with capabilities for running on GPUs.
- [DASK](https://www.dask.org/): Python library for parallel computing maintained

:::
::: {.column width="50%"}

R: 

- [gputools](https://github.com/maweigert/gputools): Provides a variety of GPU-enabled functions, including matrix operations, solving linear equations, and hierarchical clustering.
- [cudaBayesreg](https://github.com/cran/cudaBayesreg): Designed for Bayesian regression modeling on NVIDIA GPUs, using CUDA.
- [gpuR](https://github.com/cdeterman/gpuR): An R package that interfaces with both OpenCL and CUDA to allow R users to access GPU functions for accelerating matrix algebra and operations.
- [Torch for R](https://torch.mlverse.org/): An R machine learning framework based on PyTorch
- [TensorFlow for R](https://tensorflow.rstudio.com/): An R interface to a Python build of TensorFlow

:::
::::
:::

## :star: TensorFlow and PyTorch

::: { style="font-size: 0.75em"}

Installing TensorFlow and PyTorch on Hoffman2 is straightforward using the Anaconda package manager. (Check out my [Workshop on using Anaconda](https://github.com/ucla-oarc-hpc/H2HH_anaconda))


:::: {.columns}
::: {.column width="40%"}

Create a new conda environmnet with CUDA tools.

:::
::: {.column width="60%"}

```{.bash}
mkdir -pv $SCRATCH/conda
module load anaconda3/2023.03
conda create -p $SCRATCH/conda/tf_torch_gpu python=3.10 scikit-learn nvidia::cuda-toolkit=11.8.0 pandas -c nvidia -c conda-forge -c anaconda -y
conda activate $SCRATCH/conda/tf_torch_gpu
```

:::
::::
:::: {.columns}
::: {.column width="40%"}

Install TensorFlow/PyTorch with GPU support and the NVIDIA libraries
:::
::: {.column width="60%"}

```{.bash}
pip3 install tensorrt-cu11 tensorrt-cu11-bindings tensorrt-cu11-libs --extra-index-url https://pypi.nvidia.com
pip3 install tensorflow[and-cuda]==2.14
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```
    
:::
::::
:::: {.columns}
::: {.column width="40%"}
Verify the TensorFlow installation. Will only work if you are on a GPU-enabled node.
:::
::: {.column width="60%"}

```{.bash}
# TensofFlow Test:
python -c "import tensorflow as tf; print('TensorFlow is using:', ('GPU: ' + tf.test.gpu_device_name()) if tf.test.is_gpu_available() else 'CPU')"

# PyTorch Test:
python -c "import torch; print('PyTorch is using:', ('GPU: ' + torch.cuda.get_device_name(0)) if torch.cuda.is_available() else 'CPU')"
```

:::
::::
:::

## :dress: Fashion MNIST

::: {style="font-size: 0.60em" }

Explore machine learning with the ["Fashion MNIST"](https://paperswithcode.com/dataset/fashion-mnist) dataset using TensorFlow:

Approach:

- We will use TensorFlow to train a Netural Net model for predicting fashion categories.

Dataset Overview:

- :camera_flash: **Images:** 28x28 grayscale images of fashion products.
- :bar_chart: **Categories:** 10, with 7,000 images per category.
- :abacus: **Total Images:** 70,000.

![](mnist.png){width=60% fig-align="center"}

:::

## Runing Tensorflow

::: { style="font-size: 0.75em"}

Now that we have TensorFlow installed, we can run some examples to test the GPU acceleration.

Files in the `TF-Torch` folder contain examples of using TensorFlow on Hoffman2.

:::: {.columns}
::: {.column width="50%"}

Get a GPU node

:::
::: {.column width="50%"}

```{.bash}
qrsh -l h_data=40G,h_rt=1:00:00,gpu,A100,cuda=1
```   

:::
::::

:::: {.columns}
::: {.column width="50%"}

Set up your TensorFlow environment

:::
::: {.column width="50%"}

```{.bash}
module load anaconda3/2023.03
conda activate $SCRATCH/conda/tf_torch_gpu
```   

:::
::::

:::: {.columns}
::: {.column width="50%"}

Run CPU example

:::
::: {.column width="50%"}

```{.bash}
python minst-train-cpu.py
```   

:::
::::

:::: {.columns}
::: {.column width="50%"}

Run GPU example

:::
::: {.column width="50%"}

```{.bash}
python minst-train-gpu.py
```   

:::
::::

This approach provides a hands-on way to see the difference in performance when using GPUs compared to CPUs for training machine learning models.

:::


## :dna: DNA classification with PyTorch

::: { style="font-size: 0.75em"}
:::: {.columns}
::: {.column width="70%"}

DNA Sequence Classification with PyTorch

- :dart: **Objective:** Develop a model to classify DNA sequences.
- :microscope: **Gene Regions:** Segments of DNA containing codes for protein production.
- :test_tube: **Dataset Creation:** Generate random DNA sequences labeled as 'gene' or 'non-gene'.


:::
::: {.column width="30%"}
<img src="DNA.png" alt="DNA Illustration">
:::
::::
- :robot: **Model Development:** Use PyTorch to build a model predicting the presence of 'gene' regions.
- :rocket: **Leveraging GPUs:** Utilize the parallel processing power of GPUs for efficient training.
:::

## :running: Running PyTorch

::: { style="font-size: 0.85em"}

With PyTorch installed in the same Anaconda environment, we can now run the DNA classification example.


:::: {.columns}
::: {.column width="50%"}

**When running PyTorch on the GPU**

:::
::: {.column width="50%"}

```{.bash}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

:::
::::
:::: {.columns}
::: {.column width="50%"}

**Force running PyTorch on the GPU**

:::
::: {.column width="50%"}

```{.bash}
device = torch.device('cpu')
```

:::
::::
:::: {.columns .fragment}
::: {.column width="50%"}

**Run example**

:::
::: {.column width="50%"}

```{.bash}
python dnatorch.py
```   

:::
::::
:::


## :dna: Rapids for Genomic Data Analysis

::: {style="font-size: 0.70em" }

:::: {.columns}
::: {.column width="50%" }

We will use [RAPIDS](https://rapids.ai/) for genomic data analysis. RAPIDS is a popular platform to run data workflows, tasks, and manipulations, as well as, machine learning on GPUs.

:::
::: {.column width="50%" }

![](rapids.png){width=30% fig-align="center"}

:::
::::

We will

- Applying conditions to filter dataframes based on depth, quality, and allele frequency.
- Grouping data by chromosome and calculating mean statistics for depth, quality, and allele frequency.
- Speed comparison of these operations on GPU versus CPU.

:::

## :hammer: Install Rapids

::: {style="font-size: 0.85em" }

- RAPIDS: A suite of open-source software libraries and APIs built on CUDA to enable execution of end-to-end data science and analytics pipelines on GPUs.
- cuDF: Part of the RAPIDS ecosystem, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.

Lets add Rapids to our environment

```{.bash}
module load anaconda3/2023.03
conda create -p $SCRATCH/conda/myRapids -c rapidsai -c conda-forge -c nvidia  \
    rapids=24.04 python=3.10 cuda-version=11.8 -y
conda activate $SCRATCH/conda/myRapids
```

:::

## :mag: Running Rapids

::: {style="font-size: 0.70em" }

Navigate GPU-accelerated data manipulation with cuDF:

Files in the `rapids` folder 

- `rapids_analysis-gpu.py` - GPU version 
- `rapids_analysis-cpu.py` - CPU version

The `rapid_analysis.job` will submit the job to the Hoffman2 cluster.

In this file, the line `#$ -l gpu,V100` will submit this job to the V100 GPU nodes.

:::: {.columns .fragment}
::: {.column width="50%" }

Running Rapids

:::
::: {.column width="50%" }

```{.bash}
qsub rapids_analysis.job
```

:::
::::
:::

## :droplet: H2O.ai ML Example

::: {style="font-size: 0.70em" }
Explore machine learning with H2O.ai using the [Combined Cycle Power Plant](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant) dataset:

:::: {.columns}
::: {.column width="50%" }

- [H2O.ai](https://h2o.ai/) is an open-source platform for machine learning and AI.
- We will work through an example from [H2o-tutorials](https://github.com/h2oai/h2o-tutorials).
- :star: **Objective:** Predict the energy output of a Power Plant using temperature, pressure, humidity, and exhaust vacuum values.
- This example, we will use the R API, but H2O.ai has a Python API as well
- We will use XGBoost, a popular gradient boosting algorithm, to train the model.

:::
::: {.column width="50%" }
![](powerplant.png){width=90%}
:::
::::
:::

## :rocket: Instaling H2O.ai

::: {style="font-size: 0.85em" }

We will use R and install the H2O.ai package to run the example.

- Setting up the environment

``` {.bash}
module load cuda/11.8 
module load gcc/10.2.0
module load R/4.3.0
```

- Installing H2O.ai in R

```{.R}
mkdir -pv $R_LIBS_USER
R -q -e 'install.packages(c("RCurl", "jsonlite"), repos = "https://cran.rstudio.com")'
R -q -e 'install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))'
```

:::

## :running: Running H2O.ai

::: {style="font-size: 0.85em" }

In the `h2oai` folder, the `h2oaiXGBoost.R` script the code to run XGBoost on the Combined Cycle Power Plant dataset.

:::: {.columns}
::: {.column width="50%" }

The `h2oML-gpu.job` file will submit the job to a **GPU** node.

:::
::: {.column width="50%" }

```{.bash}
qsub h2oML-gpu.job
```   

:::
::::
:::: {.columns}
::: {.column width="50%" }

The `h2oML-cpu.job` file will submit the job to a **CPU** node.

:::
::: {.column width="50%" }

```{.bash}
qsub h2oML-cpu.job
```

:::
::::

The H2O.ai functions will automatically detect the GPU and use it for training.

:::

# :tada: Wrap up

::: { style="font-size: 0.80em" }

Hoffman2 has the resources and tools to help you leverage the power of GPUs for your research. :star:

Main Takeaways:

- Use `-l gpu` option to reserve a GPU node
- Compile GPU optimize code with CUDA
- Understand how to use your software can efficiently use GPUs
- Use Python and R packages for GPU computing

:::

## :clap: Thanks for Joining! :heart:

::: { style="font-size: 0.60em" }

Questions? Comments?

- :email: [cpeterson\@oarc.ucla.edu](mailto:cpeterson@oarc.ucla.edu){.email}

- :spiral_calendar: Look at for more [Hoffman2 workshops](https://idre.ucla.edu/calendar)


:::{ style="text-align: center" }

<img src="padfoot.jpeg"/ width="40%" height="40%">

:::
:::

# RPylab demo

## RPylab

::: { style="font-size: 0.80em" }

This is an experimental setup that I made that can run both RStudio and Jupyter on Hoffman2.

This environment has many loaded packages (mostly data science related)

A lot of these packages are optimized with Intel's OneAPI with MKL and GPU support

:::: {.columns}
::: {.column width="50%" }

This is built using Docker and can be ran on any system with Apptainer

:::
::: {.column width="50%" }

```{.bash}
apptainer pull docker://ghcr.io/charliecpeterson/rpylab:rpylab-R4.3.3-python-3.10.10-oneapi-gpu
```

:::
::::

This is a pretty large container so it may some time to download (I already have it on Hoffman2). I'm working on some minimal versions without the many packages and well as non-GPU versions.

::: {.callout-warning}
This is still a work in progress 
:::

:::

## Running RStudio

::: { style="font-size: 0.80em" }

This RStudio has TensorFlow and Torch for R installed with GPU support and MKL as well as many data science related R packages.

You can also run Python within this. Same Python as with Jupyter.

- Making tmp files for Rstudio

```{.bash}
mkdir -pv $SCRATCH/rstudiotmp/var/lib
mkdir -pv $SCRATCH/rstudiotmp/var/run
mkdir -pv $SCRATCH/rstudiotmp/tmp
```

- Start RStudio

```{.bash}
apptainer run --nv \
      -B $SCRATCH/rstudiotmp/var/lib:/var/lib/rstudio-server \
      -B $SCRATCH/rstudiotmp/var/run:/var/run/rstudio-server \
      -B $SCRATCH/rstudiotmp/tmp:/tmp \
         $H2_CONTAINER_LOC/rpylab_rpylab-R4.3.3-python-3.10.10-oneapi-gpu.sif rstudio
```

- Port forward

```{.bash}
ssh -L 8787:COMPUTENODE:8787 USERNAME@hoffman2.idre.ucla.edu
```

- Open web browser

```{.bash}
http://localhost:8787
```

:::

## Running Jupyter

::: { style="font-size: 0.80em" }

This Jupyter also has TensorFlow and PyTorch installed with GPU support and MKL. There is also a R kernel in this Jupyter (same R from RStudio).

- Start Jupyter

```{.bash}
apptainer run --nv \
         $H2_CONTAINER_LOC/rpylab_rpylab-R4.3.3-python-3.10.10-oneapi-gpu.sif jupyter
```

- Port forward

```{.bash}
ssh -L 8888:COMPUTENODE:8888 USERNAME@hoffman2.idre.ucla.edu
```

- Open web browser

```{.bash}
http://localhost:8888
```

:::

## Non-interactive R/Python

::: { style="font-size: 0.80em" }

- R

```{.bash}
apptainer run --nv \
         $H2_CONTAINER_LOC/rpylab_rpylab-R4.3.3-python-3.10.10-oneapi-gpu.sif Rscript myscript.R
```

- Python

```{.bash}
apptainer run --nv \
         $H2_CONTAINER_LOC/rpylab_rpylab-R4.3.3-python-3.10.10-oneapi-gpu.sif python myscript.py
```

:::

